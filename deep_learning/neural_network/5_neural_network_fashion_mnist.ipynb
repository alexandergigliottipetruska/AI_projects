{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion MNIST Neural Network\n",
        "\n",
        "Undertaking a classification task, the aim of this project is to develop skills in building neural networks from scratch, image preprocessing, and data augmentation. The dataset used is the Fashion MNIST dataset, which requires classifying instances into one of 10 different classes representing different clothing articles.\n",
        "\n",
        "The key learning goals of this project understanding backpropagation, loss, training loop, and gradients. Nevertheless, other features will be added that will develop other skills as well.\n",
        "\n",
        "**Main Objectives**:\n",
        "- Importing the Data\n",
        "- Cleaning the Data\n",
        "- Train/Test/Split\n",
        "- Exploratory Data Analysis (EDA) and Visualization\n",
        "- Preprocessing the Data\n",
        "- Data Augmentation\n",
        "- Training the Model\n",
        "- Hyperparameter Tuning\n",
        "- Test set evaluation\n",
        "- Metrics for performance - F1 score, precision, recall, confusion_matrix.\n",
        "- Finding out what kind of images the model's most confident wrong and correct predictions corresponded to, as well as it's most uncertain predictions.\n",
        "\n",
        "**Extra**:\n",
        "- Implement a **Neural Network** from scratch.\n",
        "- The network must have 1 input layer, 2 hidden layers, and an output layer.\n",
        "- Implement the forward propagation and backpropagation algorithms.\n",
        "- Use mini-batch gradient descent.\n",
        "- Implement the Adam optimizer, dropout, and layer normalization.\n",
        "- Add zerograd\n",
        "- Regularization with weight decay.\n",
        "- Softmax + Categorical Cross-Entropy\n",
        "- Early Stopping\n",
        "- Learning rate scheduler and momentum\n",
        "- Visualize loss curves\n",
        "- Implement ReLU\n",
        "- Modular design: Linear, ReLU, Dropout, and Softmax as separate classes.\n"
      ],
      "metadata": {
        "id": "DRnffAxX96MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KPTKkriF-ESN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Implementation\n",
        "\n",
        "The implementation of a non-modular **Neural Network**. This neural network will consist of an input layer, a hidden layers, and an output layer. Forward pass and backpropagation will be based on this model architecture, but this will change with the modular setup.\n",
        "\n",
        "Mini-batch gradient descent with Adam will be used for optimization, while dropout and weight decay will be used as regularization.\n",
        "\n",
        "Early stopping, learning rate scheduler, and momentum will all be used to help convergence. Layer normalization will be added as well.\n",
        "\n",
        "Lastly, the softmax and ReLU activation functions have been implemented.\n",
        "\n",
        "For a single datapoint, here are the forward propagation and backpropagation algorithms:\n",
        "\n",
        "**Forward Propagation** (datapoint):\n",
        "$$m = W^{(1)}x + b^{(1)}$$\n",
        "$$h = ReLU(m)$$\n",
        "$$z = W^{(2)}h + b^{(2)}$$\n",
        "$$y = softmax(z)$$\n",
        "$$L = L_{CE}(y, t)$$<br>\n",
        "\n",
        "**Backpropagation** (datapoint):\n",
        "$$\\overline{z}=y-t$$\n",
        "$$\\overline{W^{(2)}}=\\overline{z}(h)^T$$\n",
        "$$\\overline{b^{(2)}}=\\overline{z}$$\n",
        "$$\\overline{H}=\\overline{z}W^{(2)}$$\n",
        "$$\\overline{M}=\\overline{h}*ReLU'(m)$$\n",
        "$$\\overline{W^{(1)}}=\\overline{m}(x)^T$$\n",
        "$$\\overline{b^{(1)}}=\\overline{m}$$\n",
        "\n",
        "For a batch, the algorithms are formulated as follows:\n",
        "\n",
        "**Forward Propagation**:\n",
        "$$M = X(W^{(1)})^T+b^{(1)}$$\n",
        "$$H = ReLU(M)$$\n",
        "$$Z = H(W^{(2)})^T+b^{(2)}$$\n",
        "$$Y = softmax(Z)$$\n",
        "$$E = \\frac{1}{N}\\sum_{i}L_{CE}(y^{(i)}, t^{(i)})$$\n",
        "\n",
        "**Backpropagation:**\n",
        "$$\\overline{Z}=\\frac{1}{N}(Y-T)$$\n",
        "$$\\overline{W^{(2)}}=(\\overline{Z})^TH$$\n",
        "$$\\overline{b^{(2)}}=(\\overline{Z})^T1$$\n",
        "$$\\overline{H}=\\overline{Z}W^{(2)}$$\n",
        "$$\\overline{M}=\\overline{H}*ReLU'(M)$$\n",
        "$$\\overline{W^{(1)}}=(\\overline{M})^TX$$\n",
        "$$\\overline{b^{(1)}}=(\\overline{M})^T1$$\n"
      ],
      "metadata": {
        "id": "hrAkrkEC8MvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "  \"\"\"\n",
        "  This is a custom implementation of a neural network with an input layer, a hidden layers, and an output layer.\n",
        "\n",
        "  Parameters:\n",
        "  - self.dropout\n",
        "  - self.learning_rate:\n",
        "  - self.h: number of units in first hidden layer\n",
        "  - self.regularization: specify the kind of regularization (TO DO)\n",
        "  - self.lamb: the regularization strength\n",
        "  - self.batch_size: the mini-batch size\n",
        "  - self.momentum:\n",
        "  - self.epochs: the number of epochs to train for\n",
        "  - self.W1: the weights for the first hidden layer\n",
        "  - self.W2: the weights for the second hidden layer\n",
        "  - self.b1: the bias for the first hidden layer\n",
        "  - self.b2: the bias for the second hidden layer\n",
        "  - self.H: stores the value of the second hidden layer for backprop\n",
        "  - self.M: stores the value of the first hidden layer for backprop\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, dropout=0.8, learning_rate=0.01, h1=64, regularization=None, lamb=0.2, batch_size=32, momentum=0.9, epochs=100):\n",
        "    self.W1 = None\n",
        "    self.W2 = None\n",
        "    self.b1 = None\n",
        "    self.b2 = None\n",
        "    self.momentum = momentum\n",
        "    self.batch_size = batch_size\n",
        "    self.dropout = dropout\n",
        "    self.alpha = learning_rate\n",
        "    self.regularization = regularization\n",
        "    self.lamb = lamb\n",
        "    self.h1 = h1 # no. of units in the hidden layer\n",
        "    self.epochs = epochs\n",
        "    self.H = None # hidden layer\n",
        "    self.M = None # input layer\n",
        "\n",
        "  def fit(self, X, T):\n",
        "    # constants for xavier initialization\n",
        "    c1 = xavier_initialization(X.shape[1], self.h1)\n",
        "    c2 = xavier_initialization(self.h1, T.shape[1])\n",
        "\n",
        "    # Weight initialization\n",
        "    self.W1 = np.random.uniform(low=-c1, high=c1, size=(self.h1, X.shape[1]))\n",
        "    self.W2 = np.random.uniform(low=-c2, high=c2, size=(T.shape[1], self.h1))\n",
        "    self.b1 = np.zeros((self.h1, 1))\n",
        "    self.b2 = np.zeros((T.shape[1], 1))\n",
        "\n",
        "\n",
        "    # Forward propagation -- add training loop here\n",
        "    for epoch in range(epochs):\n",
        "      # Forward propagation\n",
        "      Y = forward_prop(self, X)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = compute_loss(self, Y, T)\n",
        "\n",
        "      W1_bar, W2_bar, b1_bar, b2_bar = backpropagation(self, X, Y, T, loss)\n",
        "      gradient_descent(self, W1_bar, W2_bar, b1_bar, b2_bar)\n",
        "\n",
        "  def forward_prop(self):\n",
        "    # Forward pass\n",
        "    M = X@(self.W1.T) + self.b1\n",
        "    H = self.relu(M)\n",
        "    Z = H@(self.W2.T) + self.b2\n",
        "    Y = self.softmax(Z)\n",
        "\n",
        "    # Store M and H\n",
        "    self.H = H\n",
        "    self.M = M\n",
        "\n",
        "    return Y\n",
        "\n",
        "\n",
        "  def backpropagation(self, X, Y, T, loss):\n",
        "    # Backpropagation\n",
        "    Z_bar = (1/N)*(Y - T)\n",
        "    W2_bar = (Z_bar.T)@self.H\n",
        "    b2_bar = Z_bar\n",
        "    H_bar = (Z_bar.T)@self.W2\n",
        "    M_bar = H_bar@self.relu_derivative(M)\n",
        "    W1_bar = (M_bar.T)@X\n",
        "    b1_bar = M_bar\n",
        "\n",
        "    return W1_bar, W2_bar, b1_bar, b2_bar\n",
        "\n",
        "  def gradient_descent(self, W1_bar, W2_bar, b1_bar, b2_bar):\n",
        "    self.W1 = self.W1 - self.alpha*W1_bar\n",
        "    self.W2 = self.W2 - self.alpha*W2_bar\n",
        "    self.b1 = self.b1 - self.alpha*b1_bar\n",
        "    self.b2 = self.b2 - self.alpha*b2_bar\n",
        "\n",
        "  def compute_loss(self, Y, T):\n",
        "    # Computes cost - categorical cross-entropy\n",
        "    return (1/N)*(-(T.T)@np.log(Y))\n",
        "\n",
        "  def Adam_opitimizer(self):\n",
        "    pass\n",
        "\n",
        "  def xavier_initialization(self, n_inputs, n_outputs):\n",
        "    return np.sqrt(6/(n_inputs+n_outputs))\n",
        "\n",
        "  def predict(self):\n",
        "    M = X@(self.W1.T) + self.b1\n",
        "    H = self.relu(M)\n",
        "    Z = H@(self.W2.T) + self.b2\n",
        "    Y = self.softmax(Z)\n",
        "    return np.argmax(Y, axis=1)\n",
        "\n",
        "  def relu_derivative(self, o):\n",
        "    return np.piecewise(o, [o <= 0, o > 0], [0, 1])\n",
        "\n",
        "  def relu(self, o):\n",
        "    return np.maximum(0, o)\n",
        "\n",
        "  def softmax(self, o):\n",
        "    # Softmax function implementation, the keepdims is used for broadcasting purposes.\n",
        "    z =  np.exp(o) / np.sum(np.exp(o), axis=1, keepdims=True)\n",
        "    return z\n",
        "\n",
        "  def mini_batch(self, t, X, N):\n",
        "    batches = {}\n",
        "    n_batches = N // self.batch_size\n",
        "    # Create batches\n",
        "    for i in range(n_batches):\n",
        "        batches[i] = [X[i*self.batch_size:(i+1)*self.batch_size], t[i*self.batch_size:(i+1)*self.batch_size]]\n",
        "\n",
        "    # Last batch should be compiled into its own batch, even if it's less than batch size\n",
        "    if N % self.batch_size != 0 :\n",
        "        batches[n_batches] = [X[n_batches*self.batch_size:], t[n_batches*self.batch_size:]]\n",
        "\n",
        "    return batches\n",
        "\n",
        "  def layer_normalization(self):\n",
        "    pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Spl0bU8cAqtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular Implementation of a Neural Network\n",
        "\n",
        "Divides the previous implementation k into several classes that can be combined to form the entire network. The main purpose of this is start writing modular code and create reusable classes for further projects."
      ],
      "metadata": {
        "id": "TwlLZ61Xp0Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRXAdADc9LV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the Data"
      ],
      "metadata": {
        "id": "u5GAHLEJqCs3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5l6P7TD5qDv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Test/Split"
      ],
      "metadata": {
        "id": "i5qXZ5l3qEKG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-5dxRXLqGj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "ysnExHoQqHF3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hCcx1b2vqJF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Common techniques for augmenting images are the following:\n",
        "- tbd"
      ],
      "metadata": {
        "id": "jXykmB68qM4d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKe43OkyqP2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "87qr4mIbqJmR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-r0UUqB6qLzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Training the Model"
      ],
      "metadata": {
        "id": "t9qe4wPrqU4b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj2-uIx8qWMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Lj_CAXmQqXGE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDsdr71dqY1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set Evaluation"
      ],
      "metadata": {
        "id": "ZbK_RhSRqZUR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFc7KYI8qa4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Metrics"
      ],
      "metadata": {
        "id": "SqsiJ1YwqdJk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZQPsMsAqfyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Model Prediction"
      ],
      "metadata": {
        "id": "w_sxAk5cqgMG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sm28vBbpqiTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing of Implementations"
      ],
      "metadata": {
        "id": "66TYn02VqiqL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkVPfjA4qken"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}